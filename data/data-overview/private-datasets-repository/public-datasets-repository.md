# Public Datasets Repository

A read-only collection of sample datasets datasets are provided for free for use within Gradient.

* For **Notebooks**, they are available in the directory `/datasets`, e.g., `/datasets/mnist`.
* For **Workflows**, they are in the Gradient namespace, e.g., in YAML, `ref: gradient/mnist`.

## List of Public Datasets

<table>
  <thead>
    <tr>
      <th style="text-align:left">Name &amp; Path</th>
      <th style="text-align:left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:left">
        <p><em><b>Fast.ai</b></em>
        </p>
        <p><code>/datasets/fastai/</code>
        </p>
        <p><code>ref: gradient/fastai</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>Paperspace&apos;s Fast.ai template is built for getting up and running
          with the enormously popular Fast.ai online MOOC called Practical Deep Learning
          for Coders.</p>
        <p></p>
        <p>Source: <a href="https://registry.opendata.aws/">https://registry.opendata.aws/</a> (previously
          <a
          href="http://files.fast.ai/data/">http://files.fast.ai/data/</a>)</p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>LSUN</b></em>
        </p>
        <p><code>/datasets/lsun/</code>
        </p>
        <p><code>ref: gradient/lsun</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>Contains around one million labeled images for each of 10 scene categories
          and 20 object categories.</p>
        <p>
          <br />Source: <a href="http://www.yf.io/p/lsun">http://www.yf.io/p/lsun</a>
        </p>
        <p>(was http://lsun.cs.princeton.edu/2017; link no longer active)</p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>MNIST</b></em>
        </p>
        <p><code>/datasets/mnist/</code>
        </p>
        <p><code>ref: gradient/mnist</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>The MNIST database of handwritten digits, available from this page, has
          a training set of 60,000 examples, and a test set of 10,000 examples</p>
        <p>
          <br />Source: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>
        </p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>COCO</b></em>
        </p>
        <p><code>/datasets/coco</code>
        </p>
        <p><code>ref: gradient/coco</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>COCO is a large-scale object detection, segmentation, and captioning dataset.</p>
        <p>
          <br />Source: <a href="http://cocodataset.org/">http://cocodataset.org/</a>
        </p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>Selfie</b></em>
        </p>
        <p><code>/datasets/selfie</code>
        </p>
        <p><code>ref: gradient/selfie</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>Selfie dataset contains 46,836 selfie images annotated with 36 different
          attributes divided into several categories.</p>
        <p>
          <br />Source: <a href="https://www.crcv.ucf.edu/data/Selfie/">https://www.crcv.ucf.edu/data/Selfie/</a>
        </p>
        <p>(was http://crcv.ucf.edu/data/Selfie )</p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>StyleGAN</b></em>
        </p>
        <p><code>/datasets/stylegan</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>StyleGAN is a Style-Based Generator Architecture for Generative Adversarial
          Networks. This dataset allows for photographs of people to be produced
          by the generator.</p>
        <p>
          <br />Source: <a href="https://github.com/NVlabs/stylegan">https://github.com/NVlabs/stylegan</a>
        </p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>OpenSLR</b></em>
        </p>
        <p><code>/datasets/openslr</code>
        </p>
        <p><code>ref: gradient/openslr</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>Open Speech and Language Resources. This is dataset number 12, the LibriSpeech
          ASR corpus.</p>
        <p>
          <br />Source: <a href="https://www.openslr.org/resources.php">https://www.openslr.org/resources.php</a>
        </p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>Self Driving Demo</b></em>
        </p>
        <p><code>/datasets/self-driving-demo-data</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>A dataset by comma.ai that includes over 33 hours of commute on California&apos;s
          I280 freeway.</p>
        <p>
          <br />Source: <a href="https://github.com/commaai/comma2k19">https://github.com/commaai/comma2k19</a>
        </p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>Sentiment140</b></em>
        </p>
        <p><code>/datasets/sentiment140</code>
        </p>
        <p><code>ref: gradient/sentiment140</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>Sentiment140 allows you to discover the sentiment of a brand, product,
          or topic on Twitter.</p>
        <p>
          <br />Source: <a href="http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip">http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip</a>
        </p>
      </td>
    </tr>
    <tr>
      <td style="text-align:left">
        <p><em><b>Tiny-imagenet-200</b></em>
        </p>
        <p><code>/datasets/tiny-imagenet-200</code>
        </p>
        <p><code>ref: gradient/tiny-imagenet-200</code>
        </p>
      </td>
      <td style="text-align:left">
        <p>A subset of the ImageNET dataset created by the Stanford CS231n course.
          It spans 200 image classes with 500 training examples per class. It also
          has 50 validation and 50 test examples per class.</p>
        <p>
          <br />Source: <a href="http://cs231n.stanford.edu/tiny-imagenet-200.zip">http://cs231n.stanford.edu/tiny-imagenet-200.zip</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>

